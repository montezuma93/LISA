

1) Neurally-plausible WM capacity vs. Unlimited WM capacity:  LISAs algorithm for establishing systematic asynchrony of firing between separate SPs is based on inhibitory connections between SPs: Because SPs inhibit one another, they fire out of synchrony.  This neurally-plausible basis for establishing asynchrony of firing has the very desirable property (from a theoretical perspective, i.e., as a model of human cognition) that it is intrinsically capacity-limited: Only a finite number of SPs can be active simultaneously and successfully fire out of synchrony.  This capacity limit is very successful as a model of human WM capacity limits.  When you choose the Neurally-plausible WM capacity option, it is this algorithm for establishing asynchrony that you are choosing.

At the same time, however, it can be useful to relax these WM capacity limits, for example, to see what the architecture is capable of in principle (i.e., without WM capacity limits: see, e.g., Boys&Dogs.sym) or to use LISA as an architecture for pure AI. For this purpose the Unlimited WM capacity option is useful.  This option abandons the neurally-plausible algorithm for establishing asynchrony of firing in favor of a decidedly implausible algorithm: Make a list of SPs and just fire them in order.  The resulting algorithm has no capacity limit, is not neurally or cognitively plausible, and has the capacity to make LISA a lot smarter than people are.  My own simulations with this option suggest that the model has terrific promise as a pure AI engine.  As of this writing (6/27/07) these routines have not been fully integrated with groups and may not work properly with groups.  

2) Semantic Noise:  The value of this parameter, multiplied by a random number between 0 and 1, is added to the input to each semantic unit on each iteration.  The default value of this parameter is zero.  However, with non-zero values, it can be used for symmetry breaking, for testing the robustness of the model’s various algorithms to noise, or for simulating the effects of distraction, fatigue or inattention.

3) Semantic Death Rate:  The value of this parameter is the probability that any given connection from a semantic unit to an object or predicate unit will be randomly set to zero at the beginning of a run (i.e., simulating the “death” of the semantic connection).  The default value of this parameter is zero.  This parameter can be used very effectively to simulate the effects of temporal variant fronto-temporal degeneration (i.e., temporal brain degeneration or damage; see Morrison et al., 2004). 

4) Attention:  This parameter determines the degree to which a proposition’s importance and support from other propositions influence the likelihood of its being chosen to fire when firing is either random (command Order=[ R ( n p ) ] under Sequence in the sym file) or group-based (command Order=[ Gi ( n p ) ]).  Its default value is 1.0.  With values less than 1.0, it can be used to simulate the effects of normal aging on analogical reasoning performance (see Viskontas et al., 2004).   With values greater than one?  We haven’t yet tried it.

5) Driver Inhibition:  This parameter modulates the ability of SPs in the driver to inhibit one another and thus to establish asynchrony of firing.  It only has an effect under Neurally-plausible WM (parameter 1 above).  Its default value is 1.0.  With values less than 1, it may be useful for simulating aspects of cognitive development (Hummel & Holyoak, 1997), brain damage, inattention, stress and other factors known to influence frontal lobe function.

6) Recipient Inhibition: This parameter modulates the ability of units in recipient analogs to inhibit one another and thus to establish clean mappings to driver units.  Its default value is 1.0.  With values less than 1, it may be useful for simulating aspects of cognitive development (Hummel & Holyoak, 1997), brain damage, inattention, stress and other factors known to influence frontal lobe function.

7) Hebb Learning Rate:  This parameter modulates the rate at which mapping connections (“Hebb connections” in the code) learn weights in response to their buffers.  Its default value in the current code is 1.0.  Historically (e.g., Hummel & Holyoak, 1997, 2003) its default value has been 0.9. 

 8) Bail when recip settles:  This parameter only works under Unlimited WM capacity (parameter 1 above).  When it is false, each SP in the driver runs for a fixed number of iterations before moving on to the next SP (parameter (13) Phase duration, under Dangerous parameters, below).  When it is true, the driver moves on from the current SP to the next SP 10 iterations after the units in the recipient have settled (i.e., their activations have stopped changing).  This parameter can be used to derive response-time estimates from LISA and also to increase (slightly) the speed with which the model operates by trimming iterations off the time it takes to run each SP.  In future versions of the model, it will hopefully be an option to set this parameter to True under both Unlimited and Neurally-plausible WM capacity.

9) H&H 97/03 Mapping Algorithm vs. Hummel & Green (Vers142) Mapping Algorithm:  The LISA model published by Hummel and Holyoak (1997, 2003) uses a very simple algorithm for converting mapping connection buffers into connection weights.  Long about 2003 or 2004, John Hummel and (then) grad student, Collin Green (now at NASA-Ames in Northern CA), developed a much more sophisticated (albeit much more complex) algorithm based on Hummel & Holyoak’s (1997, 2003) idea that mapping connections are not implemented neurally as connections (i.e., synapses) per se, but rather as neurons in frontal cortex with rapidly-modifiable response properties.  (Unfortunately, we have not yet published this new algorithm.  But we’re working on it.  Slowly.)   Choosing the H&H9703 value of this parameter causes the model to use the old version of the mapping algorithm; choosing the H&GVers142 value of this parameter (the default value) causes it to use the new algorithm. 

10) SSL Threshold:  This parameter (default value = 0.7) determines the proportion of a target analog that must map to the source before LISA will license self-supervised learning.  This parameter can only affect the model’s behavior when the SSL_OK command is issued in the Sequence section of the sym file.

11) Within-group support:  By default, proposition in the same group in an analog support one another (i.e., in order to influence on anothers probably of being chosen randomly to fire; see Hummel & Holyoak, 1997, 2003).  This parameter determines the strength of that support.  Its default value is 1.0.

50) Hummel & Holyoak 03 Parameter Suite:  When chosen, this option sets the model’s parameters to mimic as closely as possible the parameter values used in Hummel & Holyoak’s (2003) version of the model.

51) Default Parameter Suite: When chosen, this option sets the model’s parameters to the current default settings.